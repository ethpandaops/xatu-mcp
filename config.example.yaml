# ethpandaops MCP Server Configuration
# Copy this file to config.yaml and customize for your environment.
# Environment variables can be substituted using ${VAR_NAME} syntax.

server:
  host: "0.0.0.0"
  port: 2480
  base_url: "http://localhost:2480"

# Plugin configuration
# Each key maps to a compiled-in plugin. Plugins without config are skipped.
plugins:
  # ClickHouse configuration
  # Direct connections to ClickHouse clusters for data queries (HTTP/HTTPS only).
  clickhouse:
    clusters:
      # Main Xatu cluster - raw event data
      - name: "xatu"
        description: |
          Main Xatu ClickHouse cluster with raw Ethereum event data.
          Contains beacon chain events (blocks, attestations, blob sidecars, etc.)
          and execution layer data across mainnet, sepolia, hoodi networks.
          IMPORTANT: Always filter by the table's partition key AND meta_network_name.
          Use clickhouse://tables/{table} to discover available tables and columns.
        host: "${CLICKHOUSE_XATU_HOST}"  # e.g., clickhouse.example.com or clickhouse.example.com:8443
        database: "${CLICKHOUSE_XATU_DATABASE}"
        username: "${CLICKHOUSE_XATU_USERNAME}"
        password: "${CLICKHOUSE_XATU_PASSWORD}"
        secure: true  # Use TLS (default: true)
        skip_verify: false  # Skip TLS cert verification (default: false)
        timeout: 120  # Query timeout in seconds

      # Pre-aggregated CBT data
      - name: "xatu-cbt"
        description: |
          Xatu CBT (ClickHouse Build Tool) cluster with pre-aggregated data built on top of the raw event data in the xatu cluster.
          Contains block timing analysis, attestation accuracy by entity, and
          validator performance metrics. Faster queries than raw data.
          IMPORTANT: Always filter by the table's partition key.
          Use clickhouse://tables/{table} to discover available tables.
        host: "${CLICKHOUSE_CBT_HOST}"
        database: "${CLICKHOUSE_CBT_DATABASE}"
        username: "${CLICKHOUSE_CBT_USERNAME}"
        password: "${CLICKHOUSE_CBT_PASSWORD}"
        secure: true
        timeout: 120

      # Experimental cluster for devnet data
      - name: "xatu-experimental"
        description: |
          Experimental Xatu ClickHouse cluster with devnet Ethereum event data.
          Contains beacon chain events (blocks, attestations, blob sidecars, etc.)
          and execution layer data for devnet networks only.
          IMPORTANT: Always filter by the table's partition key AND meta_network_name.
          Use clickhouse://tables/{table} to discover available tables and columns.
        host: "${CLICKHOUSE_EXPERIMENTAL_HOST}"
        database: "${CLICKHOUSE_EXPERIMENTAL_DATABASE}"
        username: "${CLICKHOUSE_EXPERIMENTAL_USERNAME}"
        password: "${CLICKHOUSE_EXPERIMENTAL_PASSWORD}"
        secure: true
        timeout: 120

    # ClickHouse schema discovery configuration
    # Discovers table schemas from configured ClickHouse clusters.
    schema_discovery:
      # enabled: true  # Defaults to true if datasources are configured
      refresh_interval: 15m  # How often to refresh schema cache

      # Datasources to discover schemas from
      datasources:
        - name: "xatu"      # References clusters[].name above
          cluster: "xatu"   # Logical cluster name for resources
        - name: "xatu-cbt"
          cluster: "xatu-cbt"
        - name: "xatu-experimental"
          cluster: "xatu-experimental"

  # Prometheus configuration
  # Direct connections to Prometheus/VictoriaMetrics instances.
  prometheus:
    instances:
      - name: "ethpandaops"
        description: |
          Prometheus/VictoriaMetrics for infrastructure metrics.
          Contains beacon node metrics, validator metrics, and system health data.
          Common labels: ingress_user, container, pod, namespace, instance.
        url: "${PROMETHEUS_URL}"  # e.g., https://prometheus.example.com
        username: "${PROMETHEUS_USERNAME}"  # Optional (for basic auth)
        password: "${PROMETHEUS_PASSWORD}"  # Optional
        skip_verify: false  # Skip TLS cert verification (default: false)
        timeout: 60  # Query timeout in seconds

  # Loki configuration
  # Direct connections to Loki instances for log queries.
  loki:
    instances:
      - name: "ethpandaops"
        description: |
          Loki cluster for beacon node and validator logs.
          Must filter by label (e.g., ingress_user, container, pod, namespace).
          Common queries: error patterns, consensus issues, peer connectivity.
        url: "${LOKI_URL}"  # e.g., https://loki.example.com
        username: "${LOKI_USERNAME}"  # Optional (for basic auth)
        password: "${LOKI_PASSWORD}"  # Optional
        skip_verify: false  # Skip TLS cert verification (default: false)
        timeout: 60  # Query timeout in seconds

# Sandbox configuration
sandbox:
  # Backend: docker (local dev) | gvisor (production)
  backend: docker
  image: "mcp-sandbox:latest"
  timeout: 60  # seconds
  memory_limit: "2g"
  cpu_limit: 1.0
  # Network for sandbox containers. In stdio mode (outside docker-compose),
  # the server auto-creates this network if it doesn't exist.
  network: "mcp-internal"
  # host_shared_path: "/tmp/mcp-sandbox"  # Docker-in-Docker: host-visible path for bind mounts

  # Sessions configuration (optional)
  # When enabled, sandbox containers persist between calls (enabled by default)
  # sessions:
  #   enabled: true
  #   ttl: 30m          # idle timeout (default: 30m)
  #   max_duration: 4h  # absolute max session lifetime (default: 4h)
  #   max_sessions: 10  # max concurrent sessions (default: 10)

# S3-compatible storage for output files
# IMPORTANT: The endpoint must be reachable from sandbox containers (on the sandbox network),
# not from the host. Use Docker service names (e.g., http://minio:9000), not localhost URLs.
storage:
  endpoint: "${S3_ENDPOINT}"  # e.g., http://minio:9000 or https://xxx.r2.cloudflarestorage.com
  access_key: "${S3_ACCESS_KEY}"
  secret_key: "${S3_SECRET_KEY}"
  bucket: "mcp-outputs"
  region: "us-east-1"
  public_url_prefix: "${S3_PUBLIC_URL}"  # e.g., https://outputs.example.com

# Authentication configuration
# When enabled, requires GitHub OAuth for HTTP transports (SSE, streamable-http).
# Stdio transport (local/Claude Desktop) never requires auth.
auth:
  enabled: false  # Set to true to require authentication

  # GitHub OAuth configuration (required when auth.enabled is true)
  # github:
  #   client_id: "${GITHUB_CLIENT_ID}"
  #   client_secret: "${GITHUB_CLIENT_SECRET}"

  # Organizations allowed to access the server (empty = all authenticated users)
  # allowed_orgs:
  #   - "ethpandaops"

  # JWT token configuration
  # tokens:
  #   secret_key: "${JWT_SECRET_KEY}"  # Required when auth is enabled

# Credential proxy configuration (optional)
# The credential proxy runs automatically to protect datasource credentials.
# Sandbox containers never receive credentials directly - they connect via the proxy.
# proxy:
#   listen_addr: ":8081"   # Address for proxy to listen on
#   token_ttl: 1h          # How long per-execution tokens are valid
#   sandbox_host: "host.docker.internal"  # Hostname/IP for sandbox containers to reach proxy

# Observability configuration
observability:
  metrics_enabled: true
  metrics_port: 31490

# Semantic search configuration (required).
# Ensure the model file exists at the configured path.
# semantic_search:
#   model_path: "models/MiniLM-L6-v2.Q8_0.gguf"  # Local dev default; in Docker image it's /usr/share/mcp/...
#   gpu_layers: 0                               # Layers to offload to GPU (0 = CPU only)
